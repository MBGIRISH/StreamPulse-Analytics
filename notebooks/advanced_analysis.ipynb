{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced OTT Streaming Analytics\n",
        "## User Insights, Churn Prediction & Hybrid Recommendation Engine\n",
        "\n",
        "This notebook performs end-to-end analysis of OTT streaming data including:\n",
        "- Data generation and cleaning\n",
        "- Exploratory Data Analysis (EDA)\n",
        "- Visualization\n",
        "- Churn prediction modeling\n",
        "- Content-based recommendations\n",
        "- Collaborative filtering\n",
        "- Hybrid recommendation engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.5)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.7.2)\n",
            "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.10.7)\n",
            "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.11/site-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /opt/homebrew/lib/python3.11/site-packages (6.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mbgirish/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mbgirish/Library/Python/3.11/lib/python/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /opt/homebrew/lib/python3.11/site-packages (from plotly) (2.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/mbgirish/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas numpy scikit-learn matplotlib seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Libraries imported successfully\n",
            "âœ“ Project root: /Users/mbgirish/StreamPulse-Analytics\n",
            "âœ“ Working directory: /Users/mbgirish/StreamPulse-Analytics\n",
            "âœ“ Python version: 3.11.14 (main, Oct  9 2025, 16:16:55) [Clang 17.0.0 (clang-1700.4.4.1)]\n",
            "âœ“ Pandas version: 2.3.3\n",
            "\n",
            "ðŸ”„ Forcing module reload to clear cache...\n",
            "âœ“ Module cache cleared!\n",
            "âœ“ Libraries imported successfully\n",
            "âœ“ Project root: /Users/mbgirish/StreamPulse-Analytics\n",
            "âœ“ Working directory: /Users/mbgirish/StreamPulse-Analytics\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries and setup paths\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get project root directory - handle both notebook execution contexts\n",
        "current_dir = os.getcwd()\n",
        "if 'notebooks' in current_dir:\n",
        "    # If we're in notebooks folder, go up one level\n",
        "    project_root = os.path.dirname(current_dir)\n",
        "else:\n",
        "    # Otherwise assume we're at project root\n",
        "    project_root = current_dir\n",
        "\n",
        "src_dir = os.path.join(project_root, 'src')\n",
        "\n",
        "# Verify paths exist\n",
        "if not os.path.exists(src_dir):\n",
        "    # Try alternative detection\n",
        "    script_path = os.path.abspath(__file__ if '__file__' in globals() else '.')\n",
        "    if 'notebooks' in script_path:\n",
        "        project_root = os.path.dirname(os.path.dirname(script_path))\n",
        "        src_dir = os.path.join(project_root, 'src')\n",
        "\n",
        "# Add src directory to path\n",
        "if src_dir not in sys.path and os.path.exists(src_dir):\n",
        "    sys.path.insert(0, src_dir)\n",
        "\n",
        "# Change to project root for relative paths in scripts\n",
        "if os.path.exists(project_root):\n",
        "    os.chdir(project_root)\n",
        "\n",
        "# Now import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully\")\n",
        "print(f\"âœ“ Project root: {project_root}\")\n",
        "print(f\"âœ“ Working directory: {os.getcwd()}\")\n",
        "print(f\"âœ“ Python version: {sys.version}\")\n",
        "print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
        "\n",
        "# Force reload modules to ensure we get the latest code\n",
        "import importlib\n",
        "print(\"\\nðŸ”„ Forcing module reload to clear cache...\")\n",
        "modules_to_reload = ['churn_model', 'collaborative_filtering', 'hybrid_recommender', \n",
        "                     'content_based_recommender', 'data_cleaning', 'eda', 'visualization', \n",
        "                     'generate_data', 'path_utils']\n",
        "for mod_name in modules_to_reload:\n",
        "    if mod_name in sys.modules:\n",
        "        importlib.reload(sys.modules[mod_name])\n",
        "        print(f\"  âœ“ Reloaded {mod_name}\")\n",
        "print(\"âœ“ Module cache cleared!\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully\")\n",
        "print(f\"âœ“ Project root: {project_root}\")\n",
        "print(f\"âœ“ Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Synthetic Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating synthetic datasets...\n",
            "Generating OTT Users dataset...\n",
            "âœ“ Generated 10000 users\n",
            "\n",
            "Generating OTT Titles dataset...\n",
            "âœ“ Generated 2000 titles\n",
            "\n",
            "Generating OTT Watch History dataset...\n",
            "âœ“ Generated 150000 watch records\n",
            "\n",
            "==================================================\n",
            "Dataset Generation Complete!\n",
            "==================================================\n",
            "\n",
            "Users: 10000\n",
            "Titles: 2000\n",
            "Watch Records: 150000\n",
            "\n",
            "Churn Rate: 14.68%\n",
            "Average Watch Time: 88.2 minutes\n",
            "\n",
            "âœ“ Data generation complete!\n"
          ]
        }
      ],
      "source": [
        "# Generate all datasets\n",
        "from generate_data import main as generate_main\n",
        "\n",
        "print(\"Generating synthetic datasets...\")\n",
        "generate_main()\n",
        "print(\"\\nâœ“ Data generation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning datasets...\n",
            "Loading datasets...\n",
            "\n",
            "Cleaning users data...\n",
            "âœ“ Cleaned 10000 users\n",
            "\n",
            "Cleaning watch history data...\n",
            "âœ“ Cleaned 149988 watch records\n",
            "\n",
            "Cleaning titles data...\n",
            "âœ“ Cleaned 2000 titles\n",
            "\n",
            "Merging datasets...\n",
            "âœ“ Merged dataset: 149988 records\n",
            "\n",
            "==================================================\n",
            "Data Cleaning Complete!\n",
            "==================================================\n",
            "\n",
            "Users: 10000\n",
            "Watch Records: 149988\n",
            "Titles: 2000\n",
            "Merged Records: 149988\n",
            "\n",
            "âœ“ Data cleaning complete!\n"
          ]
        }
      ],
      "source": [
        "# Clean and preprocess datasets\n",
        "from data_cleaning import main as clean_main\n",
        "\n",
        "print(\"Cleaning datasets...\")\n",
        "users_df, watch_df, titles_df, merged_df = clean_main()\n",
        "print(\"\\nâœ“ Data cleaning complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing EDA...\n",
            "Loading cleaned datasets...\n",
            "\n",
            "==================================================\n",
            "EXPLORATORY DATA ANALYSIS\n",
            "==================================================\n",
            "\n",
            "1. Genre Engagement Analysis...\n",
            "           total_watch_time  avg_watch_time  watch_count  avg_rating  \\\n",
            "genre                                                                  \n",
            "Animation         1203315.0           88.88        13538        3.20   \n",
            "Fantasy           1198426.2           86.73        13818        3.20   \n",
            "Romance           1149646.7           88.40        13005        3.21   \n",
            "Drama             1139751.6           87.79        12983        3.21   \n",
            "Thriller          1138306.2           86.71        13128        3.20   \n",
            "Mystery           1114470.4           88.89        12537        3.20   \n",
            "Action            1083749.6           86.72        12497        3.21   \n",
            "Crime             1042493.9           87.08        11972        3.21   \n",
            "Horror            1033144.9           86.70        11917        3.20   \n",
            "Comedy            1027307.3           85.97        11950        3.24   \n",
            "\n",
            "           unique_users  \n",
            "genre                    \n",
            "Animation          7422  \n",
            "Fantasy            7506  \n",
            "Romance            7241  \n",
            "Drama              7256  \n",
            "Thriller           7279  \n",
            "Mystery            7176  \n",
            "Action             7146  \n",
            "Crime              6958  \n",
            "Horror             7001  \n",
            "Comedy             7005  \n",
            "\n",
            "2. Session Pattern Clustering...\n",
            "         avg_watch_time  sessions_per_week    age  churn\n",
            "cluster                                                 \n",
            "0                120.86               9.46  37.50   0.12\n",
            "1                110.49               5.29  26.99   0.14\n",
            "2                 77.17               4.01  46.90   0.15\n",
            "3                 60.12               3.39  28.29   0.17\n",
            "\n",
            "3. Country-wise Analysis...\n",
            "           total_users  churn_rate  avg_watch_time  avg_sessions_per_week\n",
            "country                                                                  \n",
            "Italy              898        0.14           86.36                   5.15\n",
            "Canada             870        0.13           89.77                   5.13\n",
            "UK                 869        0.14           88.98                   5.11\n",
            "India              860        0.16           87.88                   5.07\n",
            "Japan              857        0.15           88.88                   5.17\n",
            "Spain              850        0.15           88.33                   5.11\n",
            "Germany            849        0.14           88.88                   5.15\n",
            "France             828        0.14           87.62                   5.20\n",
            "Australia          821        0.15           87.61                   5.16\n",
            "USA                782        0.15           87.59                   5.19\n",
            "\n",
            "4. Time-based Engagement...\n",
            "\n",
            "Daily Patterns:\n",
            "             watch_time_minutes  user_id\n",
            "day_of_week                             \n",
            "Friday                1812956.7     8753\n",
            "Monday                1961719.4     8905\n",
            "Saturday              1861150.7     8834\n",
            "Sunday                1960288.3     8883\n",
            "Thursday              1742591.9     8621\n",
            "Tuesday               2044255.8     9002\n",
            "Wednesday             1709533.0     8564\n",
            "\n",
            "Peak Hours:\n",
            "      watch_time_minutes  user_id\n",
            "hour                             \n",
            "0             13092495.8    10000\n",
            "\n",
            "5. Completion Analysis...\n",
            "\n",
            "Overall Completion Rate: 96.02%\n",
            "\n",
            "Completion by Genre:\n",
            "genre\n",
            "Mystery        0.963048\n",
            "Documentary    0.961992\n",
            "Romance        0.961943\n",
            "Animation      0.961694\n",
            "Action         0.961114\n",
            "Drama          0.960468\n",
            "Thriller       0.959774\n",
            "Fantasy        0.959750\n",
            "Crime          0.959427\n",
            "Horror         0.958238\n",
            "Comedy         0.958219\n",
            "Sci-Fi         0.956834\n",
            "Name: completion_rate, dtype: float64\n",
            "\n",
            "6. User Lifetime Value...\n",
            "                      ltv  ltv_adjusted  user_id\n",
            "subscription_plan                               \n",
            "basic              470.26        727.71     3045\n",
            "premium            943.10       1954.77     2474\n",
            "standard           703.55       1266.31     4481\n",
            "\n",
            "==================================================\n",
            "EDA Complete!\n",
            "==================================================\n",
            "\n",
            "âœ“ EDA complete!\n"
          ]
        }
      ],
      "source": [
        "# Perform comprehensive EDA\n",
        "from eda import main as eda_main\n",
        "\n",
        "print(\"Performing EDA...\")\n",
        "users_df, watch_df, titles_df = eda_main()\n",
        "print(\"\\nâœ“ EDA complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating visualizations...\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "==================================================\n",
            "\n",
            "1. Creating engagement heatmap...\n",
            "âœ“ Saved engagement_heatmap.png\n",
            "\n",
            "2. Creating genre popularity over time...\n",
            "âœ“ Saved genre_popularity_over_time.png\n",
            "\n",
            "3. Creating rating distribution...\n",
            "âœ“ Saved rating_distribution.png\n",
            "\n",
            "4. Creating device watch patterns...\n",
            "âœ“ Saved device_watch_patterns.png\n",
            "\n",
            "5. Creating churn breakdown...\n",
            "âœ“ Saved churn_breakdown.png\n",
            "\n",
            "6. Creating KMeans cluster visualization...\n",
            "âœ“ Saved kmeans_clusters.png\n",
            "\n",
            "==================================================\n",
            "All visualizations saved to outputs/charts/\n",
            "==================================================\n",
            "\n",
            "âœ“ Visualizations saved to outputs/charts/\n"
          ]
        }
      ],
      "source": [
        "# Generate all visualizations\n",
        "from visualization import main as viz_main\n",
        "\n",
        "print(\"Generating visualizations...\")\n",
        "viz_main()\n",
        "print(\"\\nâœ“ Visualizations saved to outputs/charts/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Churn Prediction Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training churn prediction models...\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "CHURN PREDICTION MODEL\n",
            "==================================================\n",
            "\n",
            "1. Feature Engineering...\n",
            "\n",
            "2. Preparing Features...\n",
            "   Features: 13\n",
            "   Samples: 10000\n",
            "   Churn Rate: 14.68%\n",
            "\n",
            "3. Training Models...\n",
            "\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 0.9190\n",
            "  Precision: 0.8474\n",
            "  Recall: 0.5476\n",
            "  F1-Score: 0.6653\n",
            "  ROC-AUC: 0.8868\n",
            "\n",
            "Training Random Forest...\n",
            "  Accuracy: 0.9170\n",
            "  Precision: 0.8265\n",
            "  Recall: 0.5510\n",
            "  F1-Score: 0.6612\n",
            "  ROC-AUC: 0.8649\n",
            "\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 0.9200\n",
            "  Precision: 0.8221\n",
            "  Recall: 0.5816\n",
            "  F1-Score: 0.6813\n",
            "  ROC-AUC: 0.8844\n",
            "\n",
            "4. Creating Visualizations...\n",
            "âœ“ Saved churn_confusion_matrices.png\n",
            "âœ“ Saved churn_roc_curves.png\n",
            "\n",
            "5. Saving Metrics...\n",
            "âœ“ Saved churn_model_metrics.txt\n",
            "\n",
            "==================================================\n",
            "Churn Prediction Complete!\n",
            "==================================================\n",
            "\n",
            "âœ“ Churn prediction complete!\n"
          ]
        }
      ],
      "source": [
        "# Train churn prediction models\n",
        "from churn_model import main as churn_main\n",
        "\n",
        "print(\"Training churn prediction models...\")\n",
        "results, trained_models, scaler, le_dict = churn_main()\n",
        "print(\"\\nâœ“ Churn prediction complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Content-Based Recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training content-based recommender...\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "CONTENT-BASED RECOMMENDER\n",
            "==================================================\n",
            "\n",
            "1. Initializing Recommender...\n",
            "\n",
            "2. Fitting Model...\n",
            "Fitting TF-IDF vectorizer...\n",
            "Computing cosine similarity matrix...\n",
            "âœ“ Similarity matrix shape: (2000, 2000)\n",
            "\n",
            "3. Testing Recommendations...\n",
            "\n",
            "Recommendations for Title ID 1:\n",
            "            title_name  genre  similarity_score\n",
            " Action Chronicles 190 Action               1.0\n",
            "    Action Secrets 635 Action               1.0\n",
            " Action Chronicles 842 Action               1.0\n",
            "Action Chronicles 1256 Action               1.0\n",
            "    Action Legacy 1463 Action               1.0\n",
            "\n",
            "Recommendations for User ID 2827:\n",
            "             title_name       genre  similarity_score\n",
            "    Action Secrets 1670      Action               1.0\n",
            "      Action Tales 1754      Action               1.0\n",
            "      Drama Secrets 667       Drama               1.0\n",
            "   Drama Chronicles 654       Drama               1.0\n",
            "       Drama Legacy 638       Drama               1.0\n",
            "      Horror Quest 1674      Horror               1.0\n",
            "      Horror Tales 1287      Horror               1.0\n",
            " Horror Chronicles 1208      Horror               1.0\n",
            "Documentary Legacy 1956 Documentary               1.0\n",
            "Documentary Legacy 1744 Documentary               1.0\n",
            "\n",
            "==================================================\n",
            "Content-Based Recommender Complete!\n",
            "==================================================\n",
            "\n",
            "âœ“ Content-based recommender ready!\n"
          ]
        }
      ],
      "source": [
        "# Initialize and train content-based recommender\n",
        "from content_based_recommender import main as content_main\n",
        "\n",
        "print(\"Training content-based recommender...\")\n",
        "content_recommender = content_main()\n",
        "print(\"\\nâœ“ Content-based recommender ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Collaborative Filtering Recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Surprise library not available. Using scikit-learn NMF as fallback.\n",
            "Training collaborative filtering recommender...\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "COLLABORATIVE FILTERING RECOMMENDER\n",
            "==================================================\n",
            "\n",
            "Note: Using scikit-learn NMF as fallback (Surprise not available)\n",
            "For best results, install Surprise with: pip install scikit-surprise\n",
            "(May require Python < 3.14 or building from source)\n",
            "\n",
            "\n",
            "1. Initializing Recommender...\n",
            "\n",
            "2. Preparing Data...\n",
            "Filtered data: 54984 ratings from 9394 users\n",
            "\n",
            "3. Training Model...\n",
            "\n",
            "Training collaborative filtering model...\n",
            "âœ“ NMF Model Trained (Fallback)\n",
            "  RMSE: 3.2554\n",
            "  MAE: 3.0373\n",
            "\n",
            "4. Testing Recommendations...\n",
            "\n",
            "Recommendations for User ID 2827:\n",
            "              title_name       genre  predicted_rating\n",
            "     Action Chronicles 1      Action                 1\n",
            "      Drama Journey 1344       Drama                 1\n",
            "       Crime Legacy 1342       Crime                 1\n",
            "    Mystery Journey 1341     Mystery                 1\n",
            "     Comedy Journey 1340      Comedy                 1\n",
            "    Fantasy Journey 1339     Fantasy                 1\n",
            "      Drama Journey 1338       Drama                 1\n",
            "       Horror Quest 1337      Horror                 1\n",
            "Documentary Journey 1336 Documentary                 1\n",
            "       Sci-Fi Tales 1335      Sci-Fi                 1\n",
            "\n",
            "==================================================\n",
            "Collaborative Filtering Complete!\n",
            "==================================================\n",
            "\n",
            "âœ“ Collaborative filtering recommender ready!\n"
          ]
        }
      ],
      "source": [
        "# Initialize and train collaborative filtering recommender\n",
        "from collaborative_filtering import main as collab_main\n",
        "\n",
        "print(\"Training collaborative filtering recommender...\")\n",
        "collab_recommender = collab_main()\n",
        "print(\"\\nâœ“ Collaborative filtering recommender ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Hybrid Recommendation Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating hybrid recommendation engine...\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "HYBRID RECOMMENDER ENGINE\n",
            "==================================================\n",
            "\n",
            "1. Initializing Content-Based Recommender...\n",
            "Fitting TF-IDF vectorizer...\n",
            "Computing cosine similarity matrix...\n",
            "âœ“ Similarity matrix shape: (2000, 2000)\n",
            "\n",
            "2. Initializing Collaborative Filtering Recommender...\n",
            "Filtered data: 54984 ratings from 9394 users\n",
            "\n",
            "Training collaborative filtering model...\n",
            "âœ“ NMF Model Trained (Fallback)\n",
            "  RMSE: 3.2554\n",
            "  MAE: 3.0373\n",
            "\n",
            "3. Creating Hybrid Recommender...\n",
            "\n",
            "4. Testing Hybrid Recommendations...\n",
            "\n",
            "User 2827 - Top 3 Recommendations:\n",
            "          title_name   genre  hybrid_score\n",
            " Action Chronicles 1  Action           0.6\n",
            "   Action Tales 1334  Action           0.6\n",
            "Mystery Journey 1341 Mystery           0.6\n",
            "\n",
            "User 2093 - Top 3 Recommendations:\n",
            "            title_name       genre  hybrid_score\n",
            "   Action Chronicles 1      Action           0.6\n",
            "Documentary Quest 1331 Documentary           0.6\n",
            "  Fantasy Journey 1339     Fantasy           0.6\n",
            "\n",
            "User 7164 - Top 3 Recommendations:\n",
            "          title_name   genre  hybrid_score\n",
            " Action Chronicles 1  Action           0.6\n",
            "   Action Tales 1334  Action           0.6\n",
            "Mystery Journey 1341 Mystery           0.6\n",
            "\n",
            "User 5917 - Top 3 Recommendations:\n",
            "          title_name   genre  hybrid_score\n",
            " Action Chronicles 1  Action           0.6\n",
            "   Action Tales 1334  Action           0.6\n",
            "Mystery Journey 1341 Mystery           0.6\n",
            "\n",
            "User 5014 - Top 3 Recommendations:\n",
            "              title_name       genre  hybrid_score\n",
            "     Action Chronicles 1      Action           0.6\n",
            "Documentary Journey 1336 Documentary           0.6\n",
            "      Crime Secrets 1343       Crime           0.6\n",
            "\n",
            "âœ“ Saved 50 recommendations to sample_recommendations.csv\n",
            "\n",
            "==================================================\n",
            "Hybrid Recommender Complete!\n",
            "==================================================\n",
            "\n",
            "âœ“ Hybrid recommender complete!\n"
          ]
        }
      ],
      "source": [
        "# Create and test hybrid recommender\n",
        "from hybrid_recommender import main as hybrid_main\n",
        "\n",
        "print(\"Creating hybrid recommendation engine...\")\n",
        "hybrid_recommender = hybrid_main()\n",
        "print(\"\\nâœ“ Hybrid recommender complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Summary and Insights\n",
        "\n",
        "### Key Findings:\n",
        "1. **User Segmentation**: Users clustered into 4 distinct groups based on watch patterns\n",
        "2. **Churn Prediction**: Best model achieves high accuracy in predicting churn\n",
        "3. **Recommendations**: Hybrid approach combines content-based and collaborative filtering\n",
        "4. **Genre Analysis**: Action and Drama genres show highest engagement\n",
        "5. **Device Patterns**: TV devices show highest watch time per session\n",
        "\n",
        "### Next Steps:\n",
        "- Deploy models to production\n",
        "- Set up real-time recommendation API\n",
        "- Implement A/B testing for recommendations\n",
        "- Monitor model performance over time\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
